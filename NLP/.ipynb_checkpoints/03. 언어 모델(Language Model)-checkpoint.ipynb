{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a243bf58",
   "metadata": {},
   "source": [
    "이 내용들은 전부 딥 러닝을 이용한 자연어 처리 입문 (Won Joon Yoo) [링크](https://wikidocs.net/book/2155)에서 참고했습니다.\n",
    "\n",
    "## 03. 언어 모델(Language Model)\n",
    "\n",
    "언어 모델이란 단어 시퀀스(문장)에 확률을 할당하는 모델을 말합니다. 어떤 문장들이 있을 때, 기계가 이 문장은 적절해! 이 문장은 말이 안돼! 라고 사람처럼 정확히 판단할 수 있다면, 기계의 자연어 처리의 성능이 뛰어나다고 말할 수 있습니다.\n",
    "\n",
    "이번 챕터에서는 통계를 기반한 전통적인 언어 모델(Statistical Language Model, SLM)에 대해서 학습합니다. 통계에 기반한 언어 모델은 우리가 실제 사용하는 자연어를 근사하기에는 많은 한계가 있었고, 요즘 들어 인공 신경망이 그러한 한계를 많이 해결해주면서 통계 기반 언어 모델은 많이 사용 용도가 줄었습니다. 하지만 그럼에도 통계 기반 방법론에 대한 이해는 언어 모델에 대한 전체적인 시야를 갖는 일에 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409f112",
   "metadata": {},
   "source": [
    "### 1) 언어 모델이란?\n",
    "\n",
    "언어 모델은 언어라는 현상을 모델링하고자 단어 시퀀스에 확률을 할당하는 모델입니다.\n",
    "\n",
    "언어 모델을 만드는 방법은 크게는 `통계를 이용한 방법`과 `인공신경망을 이용한 방법`으로 구분할 수 있습니다. 최근에는 통계를 이용한 방법보다는 인공 신경망을 이용한 방법이 더 좋은 성능을 보여주고 있습니다. 최근 핫한 자연어 처리의 기술인 GPT나 BERT 또한 인경 신경망 언어 모델의 개념을 사용하여 만들어졌습니다. 이번 챕터에서는 언어 모델의 개념과 언어 모델의 전통적 접근 방식인 통계적 언어 모델에 대해서 배웁니다.\n",
    "\n",
    "#### 1. 언어 모델(Language Model)\n",
    "\n",
    "언어 모델은 단어 시퀀스에 확률을 할당하는 일을 하는 모델입니다. 이를 조금 풀어서 쓰면, 언어 모델은 가장 자연스러운 단어 시퀀스를 찾아내는 모델입니다. 단어 시퀀스에 확률을 할당하기 위해서 가장 보편적으로 사용되는 방법은 언어 모델이 이전 단어들이 주어졌을 대 다음 단어를 예측하도록 하는 것입니다. \n",
    "\n",
    "다른 유형의 언어 모델로는 주어진 양쪽의 단어들로부터 가운데 비어있는 단어를 예측하는 언어 모델이 있습니다. 이는 문장의 가운데에 있는 단어를 비워놓고 양 쪽의 문맥을 통해서 빈 칸의 단어인지 맞추는 고등학교 수험 시험의 빈칸 추론 문제와 비슷합니다. 이 유형의 언어 모델은 맨 마지막 BERT 챕터에서 다루게 될 예정이고, 그때까지는 이전 단어들로부터 다음 단어를 예측하는 방식에만 집중합니다.\n",
    "\n",
    "언어 모델에 -ing를 붙인 `언어 모델링(Language Modeling)`은 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업을 말합니다. 즉, 언어 모델이 이전 단어들로부터 다음 단어를 예측하는 일은 언어 모델링입니다.\n",
    "\n",
    "자연어 처리로 유명한 스탠포드 대학교에서 언어 모델을 문법(grammar)이라고 비유하기도 합니다. 언어 모델이 단어들의 조합이 얼마나 적절한지, 또한 해당 문장이 얼마나 적합한지를 알려주는 일을 하는 것이 마치 문법이 하는 일 같기 때문입니다.\n",
    "\n",
    "#### 2. 단어 시퀀스의 확률 할당\n",
    "\n",
    "자연어 처리에서 단어 시퀀스에 확률을 할당하는 일이 왜 필요할까요? 예를 들어 보겠습니다. 여기서 대문자 P는 확률을 의미합니다.\n",
    "\n",
    "##### A. 기계 번역(Machine Translation)\n",
    "\n",
    "$P(나는 버스를 탔다) > P(나는 메론을 먹는다)$\n",
    "\n",
    "언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다.\n",
    "\n",
    "##### B. 오타 교정(Spell Correction)\n",
    "\n",
    "선생님이 교실로 부리나케  \n",
    "$P(달려갔다) > P(갈려갔다)$\n",
    "\n",
    "언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다.\n",
    "\n",
    "##### C. 음성 인식(Speech Recognition)\n",
    "\n",
    "$P(나는 메롱을 먹는다) < P(나는 메론을 먹는다)$\n",
    "\n",
    "언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단합니다.\n",
    "\n",
    "언어 모델은 위와 같이 확률을 통해 보다 적절한 문장을 판단합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a319181",
   "metadata": {},
   "source": [
    "#### 3. 주어진 이전 단어들로부터 다음 단어 예측하기\n",
    "\n",
    "언어 모델은 단어 시퀀스에 확률을 할당하는 모델입니다. 그리고 단어 시퀀스에 확률을 할당하기 위해서 가장 보편적으로 사용하는 방법은 이전 단어들이 주어졌을 때, 다음 단어를 예측하도록 하는 것입니다. 이를 조건부 확률로 표현해보겠습니다.\n",
    "\n",
    "##### A. 단어 시퀀스의 확률\n",
    "\n",
    "하나의 단어를 $w$, 단어 시퀀스를 대문자 $W$라고 한다면, $n$개의 단어가 등장하는 단어 시퀀스 $W$의 확률은 다음과 같습니다.\n",
    "\n",
    "$$P(W) = P(w_1, w_2, w_3, w_4, w_5, \\cdots, w_n)$$\n",
    "\n",
    "##### B. 다음 단어 등장 확률\n",
    "\n",
    "다음 단어 등장 확률을 식으로 표현해보겠습니다. $n-1$개의 단어가 나열된 상태에서 $n$번째 단어의 확률은 다음과 같습니다.\n",
    "\n",
    "$P(w_n|w_1, ..., w_{n-1})$     $|$의 기호는 조건부 확률(conditional probability)을 의미합니다.\n",
    "\n",
    "예를 들어 다섯번째 단어의 확률은 아래와 같습니다.\n",
    "\n",
    "$$P(w_5|w_1, w_2, w_3, w_4)$$\n",
    "\n",
    "전체 단어 시퀀스 $W$의 확률은 모든 단어가 예측되고 나서야 알 수 있으므로 단어 시퀀스의 확률은 다음과 같습니다.\n",
    "\n",
    "$$P(W) = P(w_1, w_2, w_3, w_4, ..., w_n) = \\Pi ^n_{i=1} P(w_i|w_1, ..., w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e49e7a",
   "metadata": {},
   "source": [
    "#### 4. 언어 모델의 간단한 직관\n",
    "\n",
    "비행기를 타려고 공항에 갔는데 지각을 하는 바람에 비행기를 ? 라는 문장이 있습니다. '비행기를' 다음에 어떤 단어가 오게 될지 사람은 쉽게 '놓쳤다'라고 예상 할 수 있습니다. 우리 지식에 기반하여 나올 수 있는 여러 단어들을 후보에 놓고 놓쳤다는 단어가 나올 확률이 가장 높다고 판단하였기 때문입니다.\n",
    "\n",
    "그렇다면 기계에게 위 문장을 주고, '비행기를' 다음에 나올 단어를 예측해보라고 한다면 과연 어떻게 최대한 정확히 예측할 수 있을까요? 기계도 비슷합니다. 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 확률을 예측해보고 가장 높은 확률을 가진 단어를 선택합니다. 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 등장 확률을 추정하고 가장 높은 확률을 가진 단어를 선택해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5a0e2",
   "metadata": {},
   "source": [
    "### 2) 통계적 언어 모델(Statistical Language Model, SLM)\n",
    "\n",
    "언어 모델의 전통적인 접근 방법인 통계적 언어 모델을 소개합니다. 통계적 언어 모델이 통계적인 접근 방법으로 어떻게 언어를 모델링 하는지 배워보겠습니다. 통계적 언어 모델(Statistical Language Model)은 줄여서 SLM이라고 합니다.\n",
    "\n",
    "#### 1. 조건부 확률\n",
    "\n",
    "조건부 확률은 두 확률 $P(A), P(B)$에 대해서 아래와 같은 관계를 갖습니다.\n",
    "\n",
    "$$P(B|A) = P(A, B)|P(A)$$\n",
    "$$P(A, B) = P(A)P(B|A)$$\n",
    "\n",
    "더 많은 확률에 대해서 일반화해봅시다. 4개의 확률이 조건부 확률의 관계를 가질 때, 아래와 같이 표현할 수 있습니다.\n",
    "\n",
    "$$P(A, B, C, D) = P(A)P(B|A)P(C|A, B)P(D|A, B, C)$$\n",
    "\n",
    "이를 조건부 확률의 연쇄 법칙이라고 합니다. 이제는 4개가 아닌 $n$개에 대해서 일반화를 해봅시다.\n",
    "\n",
    "$$P(x_1, x_2, ... , x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1, x_2) ... P(x_n|x_1, ... x_{n-1})$$\n",
    "\n",
    "조건부 확률에 대한 정의를 통해 문장의 확률을 구해보겠습니다.\n",
    "\n",
    "#### 2. 문장에 대한 확률\n",
    "\n",
    "문장 : `An adorable little boy is spreading smiles`의 확률  \n",
    "$P(An adorable little boy is spreading smiles)$를 식으로 표현해봅시다.\n",
    "\n",
    "각 단어는 문맥이라는 관계로 인해 이전 단어의 영향을 받아 나온 단어입니다. 그리고 모든 단어로부터 하나의 문장이 완성됩니다. 그렇기 때문에 문장의 확률을 구하고자 조건부 확률을 사용하겠습니다. 앞서 언급한 조건부 확률의 일반화 식을 문장의 확률 관점에서 다시 적어보면 문장의 확률은 각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률의 곱으로 구성됩니다.\n",
    "\n",
    "$$P(w_1, w_2, ..., w_n) = \\Pi ^n_{n=1} P(w_n|w_1, ..., w_{n-1})$$\n",
    "\n",
    "위의 문장에 해당 식을 적용해보면 다음과 같습니다.\n",
    "\n",
    "$$P(An adorable little boy is spreading smiles) = P(An) \\cdot P(adorable|An) \\dot P(little|An adorable) \\dot P(boy|An adorable little) \\dot P(is|An adorable little boy) \\dot P(spreading|An adorable little boy is) \\dot P(smiles|An adorable little boy is spreading)$$\n",
    "\n",
    "문장의 확률을 구하기 위해서 각 단어에 대한 예측 확률들을 곱합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6238030",
   "metadata": {},
   "source": [
    "#### 3. 카운트 기반의 접근\n",
    "\n",
    "문장의 확률을 구하기 위해서 다음 단어에 대한 예측 확률을 모두 곱한다는 것은 알았습니다. 그렇다면 SLM은 이전 단어로부터 다음 단어에 대한 확률은 어떻게 구할까요? 정답은 카운트에 기반하여 확률을 계산합니다.\n",
    "\n",
    "An adorable little boy가 나왔을 때, is가 나올 확률인 $P(is|An adorable little boy)$를 구해봅시다.\n",
    "\n",
    "$$P(is|An adorable little boy) = \\frac{count(An adorable little boy is)}{count(An adorable little boy)}$$\n",
    "\n",
    "그 확률은 위와 같습니다. 예를 들어 기계가 학습한 코퍼스 데이터에서 An adorable little boy가 100번 등장했는데 그 다음에 is가 등장한 경우는 30번이라고 합시다. 이 경우 $P(is|An adorable little boy)$는 30%입니다.\n",
    "\n",
    "#### 4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)\n",
    "\n",
    "언어 모델은 실생활에서 사용되는 언어의 확률 분포를 근사 모델링 합니다. 실제로 정확하게 알아볼 방법은 없겠지만 현실에서도 An adorable little bo가 나왔을 때 is가 나올 확률이라는 것은 존재합니다. 이를 실제 자연어 확률 분포, 현실에서의 확률 분포라고 명칭합시다. 기계에게 많은 코퍼스를 훈련시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표입니다. 그런데 카운트 기반으로 접근하려고한다면 갖고있는 코퍼스. 즉, 다시 말해 기계가 훈련하는 데이터는 정말 방대한 양이 필요합니다.\n",
    "\n",
    "$$P(is|An adorable little boy) = \\frac{count(An adorable little boy is)}{count(An adorable little boy)}$$\n",
    "\n",
    "예를 들어 위와 같이 $P(is|An adorable little boy)$를 구하는 경우에서 기계가 훈련한 코퍼스에 An adorable little boy is라는 단어 시퀀스가 없었다면 이 단어 시퀀스에 대한 확률은 0이 됩니다. 또는 An adorable little boy라는 단어 시쿠너스가 없었다면 분모가 이 되어 확률은 정의도지 않습니다. 그렇다면 코퍼스에 단어 시퀀스가 없다고 해서 이 확률을 0 또는 정의되지 않는 확률이라고 하는 것이 정확한 모델링 방법일까요? 아닙니다. 현실에선 An adorable little boy is라는 단어 시퀀스가 존재하고 또 문법에도 적합하므로 정답일 가능성 또한 높습니다. 이와 깉이 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 `희소 문제(sparsity problem)`라고 합니다.\n",
    "\n",
    "위 문제를 완하하는 방법으로 바로 이어서 배우게 되는 `n-gram` 언어 모델이나 이 책에서 다루지는 않지만 스무딩이나 백오프와 같"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b9b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
